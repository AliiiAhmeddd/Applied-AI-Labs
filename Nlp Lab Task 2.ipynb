{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d49247",
   "metadata": {},
   "source": [
    "Challenge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f40d31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "print(\"\\n--- Challenge 1: Stop Word Removal with Punctuation ---\")\n",
    "\n",
    "text_ch1 = \"NLP enables computers to understand human language, which is a crucial aspect of artificial intelligence!\"\n",
    "\n",
    "# 1) Remove punctuation\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "text_no_punct = text_ch1.translate(translator)\n",
    "\n",
    "# 2) Tokenize\n",
    "tokens_ch1 = text_no_punct.split()\n",
    "\n",
    "# 3) Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens_ch1 = [word for word in tokens_ch1 if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text_ch1)\n",
    "print(\"\\nText without punctuation:\")\n",
    "print(text_no_punct)\n",
    "print(\"\\nFiltered tokens (no stop words, no punctuation):\")\n",
    "print(filtered_tokens_ch1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12049f6a",
   "metadata": {},
   "source": [
    "Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00e9ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, LancasterStemmer\n",
    "\n",
    "print(\"\\n--- Challenge 2: Compare Different Stemmers ---\")\n",
    "\n",
    "text_ch2 = \"Stemming helps in reducing words to their root form, which can be beneficial for text processing.\"\n",
    "tokens_ch2 = text_ch2.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "porter_stems = [porter.stem(w) for w in tokens_ch2]\n",
    "snowball_stems = [snowball.stem(w) for w in tokens_ch2]\n",
    "lancaster_stems = [lancaster.stem(w) for w in tokens_ch2]\n",
    "\n",
    "print(\"Original tokens:\")\n",
    "print(tokens_ch2)\n",
    "\n",
    "print(\"\\nPorterStemmer:\")\n",
    "print(porter_stems)\n",
    "\n",
    "print(\"\\nSnowballStemmer:\")\n",
    "print(snowball_stems)\n",
    "\n",
    "print(\"\\nLancasterStemmer:\")\n",
    "print(lancaster_stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198662d",
   "metadata": {},
   "source": [
    "Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ceb4a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Challenge 3: Lemmatization with Different POS Tags ---\")\n",
    "\n",
    "word = \"running\"\n",
    "\n",
    "lemma_as_noun = lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "lemma_as_verb = lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Lemmatized as NOUN : {lemma_as_noun}\")\n",
    "print(f\"Lemmatized as VERB : {lemma_as_verb}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f9895",
   "metadata": {},
   "source": [
    "Challenge 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32676b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(\"\\n--- Challenge 4: Regex Tokenization ---\")\n",
    "\n",
    "text_ch4 = \"Tokenization is the process of breaking a stream of text into words, phrases, symbols, or other meaningful elements called tokens. The goal is to make it easier for computers to process natural language.\"\n",
    "\n",
    "# Pattern: words (alphanumeric) OR any non-whitespace, non-word character (punctuation)\n",
    "pattern = r\"\\w+|[^\\w\\s]\"\n",
    "\n",
    "regex_tokens = regexp_tokenize(text_ch4, pattern)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text_ch4)\n",
    "print(\"\\nRegex tokens (words + punctuation as separate tokens):\")\n",
    "print(regex_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474eff0d",
   "metadata": {},
   "source": [
    "Challenge 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbe559",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Challenge 5: Detailed POS Tagging Analysis ---\")\n",
    "\n",
    "# Reuse: sentence and pos_tags from Exercise 5\n",
    "print(\"POS tags with explanations:\")\n",
    "\n",
    "# Minimal tag explanation dictionary for tags in this sentence\n",
    "tag_explanations = {\n",
    "    \"DT\": \"Determiner\",\n",
    "    \"JJ\": \"Adjective\",\n",
    "    \"NN\": \"Noun, singular or mass\",\n",
    "    \"NNS\": \"Noun, plural\",\n",
    "    \"VBZ\": \"Verb, 3rd person singular present\",\n",
    "    \"IN\": \"Preposition or subordinating conjunction\",\n",
    "    \".\": \"Sentence terminator (punctuation)\"\n",
    "}\n",
    "\n",
    "for word, tag in pos_tags:\n",
    "    meaning = tag_explanations.get(tag, \"Tag not in explanation dict\")\n",
    "    print(f\"{word:<10} {tag:<5} -> {meaning}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6373202",
   "metadata": {},
   "source": [
    "Challenge 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ca400",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "print(\"\\n--- Challenge 6: Frequency Distribution of Bigrams ---\")\n",
    "\n",
    "# Reuse filtered_tokens from Exercise 6 (already lowercase, no stopwords, alnum only)\n",
    "bigrams_list = list(bigrams(filtered_tokens))\n",
    "\n",
    "fdist_bigrams = FreqDist(bigrams_list)\n",
    "\n",
    "print(\"Filtered tokens:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "print(\"\\nMost common 5 bigrams:\")\n",
    "for bigram, freq in fdist_bigrams.most_common(5):\n",
    "    print(f\"{bigram}: {freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
