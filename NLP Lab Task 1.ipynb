{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49b08a4",
   "metadata": {},
   "source": [
    "# This notebook demonstrates fundamental Natural Language Processing (NLP) tasks using popular Python libraries.\n",
    "# Each exercise focuses on a specific NLP technique:\n",
    "# 1. Tokenization with NLTK\n",
    "# 2. Named Entity Recognition (NER) with SpaCy\n",
    "# 3. Sentiment Analysis with TextBlob\n",
    "# 4. Text Summarization with Sumy\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Imports and setup\n",
    "# ------------------------------------------------------\n",
    "import nltk\n",
    "nltk.download('punkt')       # Required for NLTK's word tokenizer\n",
    "nltk.download('punkt_tab')   # Sometimes needed for tokenizers used by Sumy\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Install sumy if not already installed (only needed once in a notebook environment)\n",
    "# If this causes an error outside notebooks, you can run it in a terminal instead.\n",
    "!pip install sumy\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "nltk.download('stopwords')   # Sumy often uses NLTK stopwords for summarization\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Exercise 1: Tokenization with NLTK\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Exercise 1: Tokenization with NLTK ---\")\n",
    "text1 = \"Natural Language Processing enables computers to understand human language.\"\n",
    "tokens = word_tokenize(text1)  # Uses NLTK's word_tokenize function to split the text\n",
    "print(f\"Original text: '{text1}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Challenge 1: Tokenization (different tokenizers)\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Challenge 1: Comparing different NLTK tokenizers ---\")\n",
    "\n",
    "challenge_text = \"Wow! NLP is fun, isn't it? Let's try different tokenizers.\"\n",
    "\n",
    "# 1) word_tokenize (default)\n",
    "tokens_word = word_tokenize(challenge_text)\n",
    "\n",
    "# 2) wordpunct_tokenize (splits punctuation into separate tokens)\n",
    "tokens_wordpunct = wordpunct_tokenize(challenge_text)\n",
    "\n",
    "# 3) TreebankWordTokenizer (handles contractions and punctuation in a specific way)\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "tokens_treebank = treebank_tokenizer.tokenize(challenge_text)\n",
    "\n",
    "print(f\"Original text: {challenge_text}\")\n",
    "print(\"\\nword_tokenize:\")\n",
    "print(tokens_word)\n",
    "\n",
    "print(\"\\nwordpunct_tokenize (note how punctuation is split):\")\n",
    "print(tokens_wordpunct)\n",
    "\n",
    "print(\"\\nTreebankWordTokenizer (note how it splits \\\"isn't\\\" and others):\")\n",
    "print(tokens_treebank)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Exercise 2: Named Entity Recognition with SpaCy\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Exercise 2: Named Entity Recognition with SpaCy ---\")\n",
    "# Load SpaCy model - ensure 'en_core_web_sm' is downloaded (you might need: !python -m spacy download en_core_web_sm)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  # Loads a small English model for processing\n",
    "except OSError:\n",
    "    print(\"Downloading en_core_web_sm model for SpaCy...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")          # If model is not found, download it automatically\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text2 = \"Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\"\n",
    "doc = nlp(text2)  # Process the text with the loaded SpaCy model\n",
    "print(f\"Original text: '{text2}'\")\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents:  # Iterate through the detected entities\n",
    "    print(f\"  {ent.text:<25} {ent.label_}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Challenge 2: Named Entity Recognition on a new sentence\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Challenge 2: NER on a custom sentence ---\")\n",
    "\n",
    "challenge_text2 = \"Apple released the iPhone 15 in September 2023 for $799. The event took place in Cupertino, California.\"\n",
    "doc2 = nlp(challenge_text2)\n",
    "\n",
    "print(f\"Challenge text: '{challenge_text2}'\")\n",
    "print(\"Named Entities found:\")\n",
    "for ent in doc2.ents:\n",
    "    print(f\"  {ent.text:<25} {ent.label_}\")\n",
    "\n",
    "# Optional: try another sentence with less common entities\n",
    "challenge_text3 = \"OpenAI signed a 3-year research partnership with the University of Leeds in 2025 for £2 million.\"\n",
    "doc3 = nlp(challenge_text3)\n",
    "\n",
    "print(\"\\nAdditional challenge sentence:\")\n",
    "print(f\"'{challenge_text3}'\")\n",
    "print(\"Named Entities found:\")\n",
    "for ent in doc3.ents:\n",
    "    print(f\"  {ent.text:<25} {ent.label_}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Exercise 3: Sentiment Analysis with TextBlob\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Exercise 3: Sentiment Analysis with TextBlob ---\")\n",
    "text3 = \"I am extremely happy with the service provided.\"\n",
    "blob = TextBlob(text3)      # Create a TextBlob object from the text\n",
    "sentiment = blob.sentiment  # Access the sentiment property (polarity, subjectivity)\n",
    "print(f\"Original text: '{text3}'\")\n",
    "print(f\"Sentiment: {sentiment}\")  # Polarity: -1 (neg) to 1 (pos), Subjectivity: 0 (objective) to 1 (subjective)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Challenge 3: Sentiment Analysis on a review paragraph\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Challenge 3: Sentiment on a review paragraph ---\")\n",
    "\n",
    "review_text = (\n",
    "    \"The phone’s design is beautiful and the screen is bright, \"\n",
    "    \"but the battery life is disappointing and the camera often crashes. \"\n",
    "    \"Overall, I feel a bit let down after all the hype.\"\n",
    ")\n",
    "review_blob = TextBlob(review_text)\n",
    "review_sentiment = review_blob.sentiment\n",
    "\n",
    "print(f\"Review text: '{review_text}'\")\n",
    "print(f\"Polarity: {review_sentiment.polarity:.3f}\")\n",
    "print(f\"Subjectivity: {review_sentiment.subjectivity:.3f}\")\n",
    "print(\"Interpretation: Polarity > 0 is overall positive, < 0 is negative, close to 0 is neutral.\")\n",
    "\n",
    "# Try a sentence with sarcasm\n",
    "sarcasm_text = \"Yeah, waiting 3 hours for customer support was just amazing.\"\n",
    "sarcasm_blob = TextBlob(sarcasm_text)\n",
    "sarcasm_sentiment = sarcasm_blob.sentiment\n",
    "\n",
    "print(\"\\nSarcasm example:\")\n",
    "print(f\"Text: '{sarcasm_text}'\")\n",
    "print(f\"Polarity: {sarcasm_sentiment.polarity:.3f}, Subjectivity: {sarcasm_sentiment.subjectivity:.3f}\")\n",
    "print(\"Note: Simple sentiment models often struggle with sarcasm and may misclassify it.\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Exercise 4: Text Summarization with Sumy\n",
    "# ------------------------------------------------------\n",
    "print(\"\\n--- Exercise 4: Text Summarization with Sumy ---\")\n",
    "text4 = (\n",
    "    \"Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, \"\n",
    "    \"artificial intelligence, and linguistics. It enables machines to understand, interpret, and generate \"\n",
    "    \"human language, opening up a world of possibilities for applications ranging from chatbots and translation \"\n",
    "    \"services to sentiment analysis and beyond. This field involves various techniques, including machine learning, \"\n",
    "    \"deep learning, and rule-based methods, to process and analyze large amounts of text data. The goal of NLP is \"\n",
    "    \"to bridge the communication gap between humans and computers, allowing for more natural and intuitive \"\n",
    "    \"interactions. Its applications are constantly expanding, making it a critical area of research and development \"\n",
    "    \"in today's technologically driven world.\"\n",
    ")\n",
    "\n",
    "parser = PlaintextParser.from_string(text4, Tokenizer(\"english\"))  # Parse the text using Sumy's PlaintextParser\n",
    "summarizer = LsaSummarizer()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
